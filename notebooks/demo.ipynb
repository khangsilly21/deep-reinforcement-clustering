{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73daf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deep Reinforcement Clustering (DRC) - PAPER IMPLEMENTATION\n",
    "Dataset: MNIST\n",
    "Based on: Li et al., \"Deep Reinforcement Clustering\" (IEEE TMM 2023)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"Deep AutoEncoder\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dims=[500, 500, 2000], latent_dim=10):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        encoder_layers.append(nn.Linear(prev_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_recon = self.decoder(z)\n",
    "        return z, x_recon\n",
    "\n",
    "\n",
    "class DRC(nn.Module):\n",
    "    \"\"\"Deep Reinforcement Clustering\"\"\"\n",
    "    def __init__(self, input_dim=784, hidden_dims=[500, 500, 2000], \n",
    "                 latent_dim=10, n_clusters=10):\n",
    "        super(DRC, self).__init__()\n",
    "        \n",
    "        self.n_clusters = n_clusters\n",
    "        self.latent_dim = latent_dim\n",
    "        self.autoencoder = AutoEncoder(input_dim, hidden_dims, latent_dim)\n",
    "        self.cluster_centers = nn.Parameter(torch.randn(n_clusters, latent_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z, x_recon = self.autoencoder(x)\n",
    "        return z, x_recon\n",
    "    \n",
    "    def cauchy_similarity(self, z, kappa=1.0):\n",
    "        z_expanded = z.unsqueeze(1)  # [batch, 1, dim]\n",
    "        centers_expanded = self.cluster_centers.unsqueeze(0)  # [1, K, dim]\n",
    "        distances_sq = torch.sum((z_expanded - centers_expanded) ** 2, dim=2)\n",
    "        similarities = (1.0 / np.pi) * (kappa / (distances_sq + kappa**2))\n",
    "        return similarities\n",
    "    \n",
    "    def decision_probability(self, z, kappa=1.0):\n",
    "        similarities = self.cauchy_similarity(z, kappa)\n",
    "        probs = torch.sigmoid(similarities)\n",
    "        return probs\n",
    "    \n",
    "    def get_cluster_assignments(self, z):\n",
    "        probs = self.decision_probability(z)\n",
    "        return torch.argmax(probs, dim=1)\n",
    "\n",
    "\n",
    "def pretrain_autoencoder(model, train_loader, epochs=50, lr=0.001):\n",
    "    print(\"\\n=== Pretraining AutoEncoder ===\")\n",
    "    optimizer = optim.Adam(model.autoencoder.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data, _ in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            z, x_recon = model.autoencoder(data)\n",
    "            loss = criterion(x_recon, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.6f}\")\n",
    "    print(\"Pretraining completed!\")\n",
    "\n",
    "\n",
    "def initialize_cluster_centers(model, train_loader, n_clusters):\n",
    "    print(\"\\n=== Initializing Cluster Centers ===\")\n",
    "    model.eval()\n",
    "    \n",
    "    all_z = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            z, _ = model.autoencoder(data)\n",
    "            all_z.append(z.cpu())\n",
    "    \n",
    "    all_z = torch.cat(all_z, dim=0).numpy()\n",
    "    \n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=42)\n",
    "    kmeans.fit(all_z)\n",
    "    \n",
    "    model.cluster_centers.data = torch.tensor(\n",
    "        kmeans.cluster_centers_, dtype=torch.float32\n",
    "    ).to(device)\n",
    "    print(\"Cluster centers initialized!\")\n",
    "\n",
    "\n",
    "def train_drc(model, train_loader, val_loader, epochs=100, \n",
    "              lr=0.0001, gamma=0.01, v=100.0):\n",
    "    \"\"\"\n",
    "    Train Deep Reinforcement Clustering\n",
    "    Following paper's Equation (8) exactly\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Training DRC ===\")\n",
    "    print(f\"Gamma: {gamma}, Reward v: {v}\")\n",
    "    \n",
    "\n",
    "    network_params = model.autoencoder.parameters()\n",
    "    center_params = [model.cluster_centers]\n",
    "    optimizer = torch.optim.Adam(network_params, lr=lr)\n",
    "    \n",
    "    \n",
    "    criterion_recon = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'loss': [], 'recon_loss': [], 'rc_loss': [],\n",
    "        'val_acc': [], 'val_nmi': [], 'val_ari': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_rc_loss = 0\n",
    "        \n",
    "        for data, _ in train_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            batch_size = data.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            z, x_recon = model(data)\n",
    "            \n",
    "            # Reconstruction loss \n",
    "            L_rec = criterion_recon(x_recon, data)\n",
    "            \n",
    "            # Get probability\n",
    "            probs = model.decision_probability(z)\n",
    "            \n",
    "            # Action selection \n",
    "            max_probs, max_indices = torch.max(probs, dim=1)\n",
    "            \n",
    "            # Bernoulli indicator \n",
    "            p_random = torch.rand(batch_size, device=device)\n",
    "            y_ij = (max_probs > p_random).float()\n",
    "            \n",
    "            # Rewards \n",
    "            rewards = v * (2 * y_ij - 1)\n",
    "            \n",
    "            # Get selected probabilities\n",
    "            selected_probs = probs.gather(1, max_indices.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            \n",
    "            log_term = y_ij * torch.log(selected_probs ) + \\\n",
    "                       (y_ij - 1) * torch.log(1 - selected_probs)\n",
    "            \n",
    "            # Cumulative reward\n",
    "            L_rc = -gamma * torch.mean(rewards * log_term)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = L_rec + L_rc\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += L_rec.item()\n",
    "            total_rc_loss += L_rc.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_recon = total_recon_loss / len(train_loader)\n",
    "        avg_rc = total_rc_loss / len(train_loader)\n",
    "        \n",
    "        history['loss'].append(avg_loss)\n",
    "        history['recon_loss'].append(avg_recon)\n",
    "        history['rc_loss'].append(avg_rc)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            val_metrics = evaluate_model(model, val_loader)\n",
    "            history['val_acc'].append(val_metrics['ACC'])\n",
    "            history['val_nmi'].append(val_metrics['NMI'])\n",
    "            history['val_ari'].append(val_metrics['ARI'])\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "            print(f\"  Loss: {avg_loss:.4f} | Recon: {avg_recon:.5f} | RC: {avg_rc:.4f}\")\n",
    "            print(f\"  Val ACC: {val_metrics['ACC']:.4f} | NMI: {val_metrics['NMI']:.4f} | ARI: {val_metrics['ARI']:.4f}\")\n",
    "            \n",
    "            if val_metrics['ACC'] > best_val_acc:\n",
    "                best_val_acc = val_metrics['ACC']\n",
    "                print(f\"  âœ“ New best validation ACC: {best_val_acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining completed! Best Val ACC: {best_val_acc:.4f}\")\n",
    "    return history\n",
    "\n",
    "\n",
    "def predict_clusters(model, data_loader):\n",
    "    \"\"\"Predict cluster assignments\"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            z, _ = model(data)\n",
    "            preds = model.get_cluster_assignments(z)\n",
    "            all_labels.append(labels.numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_labels), np.concatenate(all_preds)\n",
    "\n",
    "\n",
    "def cluster_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate clustering accuracy using Hungarian algorithm\"\"\"\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    row_ind, col_ind = linear_sum_assignment(w.max() - w)\n",
    "    return w[row_ind, col_ind].sum() / y_pred.size\n",
    "\n",
    "\n",
    "def evaluate_clustering(y_true, y_pred):\n",
    "    \"\"\"Evaluate clustering performance\"\"\"\n",
    "    acc = cluster_accuracy(y_true, y_pred)\n",
    "    nmi = normalized_mutual_info_score(y_true, y_pred)\n",
    "    ari = adjusted_rand_score(y_true, y_pred)\n",
    "    return {'ACC': acc, 'NMI': nmi, 'ARI': ari}\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluate model on a dataset\"\"\"\n",
    "    y_true, y_pred = predict_clusters(model, data_loader)\n",
    "    return evaluate_clustering(y_true, y_pred)\n",
    "\n",
    "\n",
    "def visualize_tsne_clusters(model, test_loader, max_samples=2000):\n",
    "    \"\"\"Visualize clusters using t-SNE \"\"\"\n",
    "    print(\"\\n=== Generating t-SNE Visualization ===\")\n",
    "    model.eval()\n",
    "    \n",
    "    all_z = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            z, _ = model(data)\n",
    "            preds = model.get_cluster_assignments(z)\n",
    "            \n",
    "            all_z.append(z.cpu())\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds.cpu())\n",
    "            \n",
    "            if len(all_z) * data.size(0) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    all_z = torch.cat(all_z, dim=0)[:max_samples].numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0)[:max_samples].numpy()\n",
    "    all_preds = torch.cat(all_preds, dim=0)[:max_samples].numpy()\n",
    "    \n",
    "    print(f\"Running t-SNE on {len(all_z)} samples...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "    z_2d = tsne.fit_transform(all_z)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    # Ground truth\n",
    "    ax1 = axes[0]\n",
    "    for i in range(10):\n",
    "        mask = all_labels == i\n",
    "        ax1.scatter(z_2d[mask, 0], z_2d[mask, 1], \n",
    "                   c=[colors[i]], label=f'Class {i}', \n",
    "                   alpha=0.6, s=20, edgecolors='none')\n",
    "    ax1.set_title('Ground Truth Labels', fontsize=16, fontweight='bold')\n",
    "    ax1.legend(loc='upper right', fontsize=9, ncol=2)\n",
    "    ax1.set_xlabel('t-SNE 1', fontsize=12)\n",
    "    ax1.set_ylabel('t-SNE 2', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.2)\n",
    "    \n",
    "    # Predicted clusters\n",
    "    ax2 = axes[1]\n",
    "    for i in range(10):\n",
    "        mask = all_preds == i\n",
    "        ax2.scatter(z_2d[mask, 0], z_2d[mask, 1], \n",
    "                   c=[colors[i]], label=f'Cluster {i}', \n",
    "                   alpha=0.6, s=20, edgecolors='none')\n",
    "    ax2.set_title('DRC Predicted Clusters', fontsize=16, fontweight='bold')\n",
    "    ax2.legend(loc='upper right', fontsize=9, ncol=2)\n",
    "    ax2.set_xlabel('t-SNE 1', fontsize=12)\n",
    "    ax2.set_ylabel('t-SNE 2', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('drc_tsne_clusters.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"t-SNE visualization saved to 'drc_tsne_clusters.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_cluster_distribution(model, test_loader, title=\"MNIST Clusters\"):\n",
    "    \"\"\"\n",
    "    Create a single t-SNE plot showing cluster distribution\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Generating {title} Visualization ===\")\n",
    "    model.eval()\n",
    "    \n",
    "    all_z = []\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    max_samples = 2000\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data = data.view(data.size(0), -1).to(device)\n",
    "            z, _ = model(data)\n",
    "            preds = model.get_cluster_assignments(z)\n",
    "            \n",
    "            all_z.append(z.cpu())\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds.cpu())\n",
    "            \n",
    "            if len(all_z) * data.size(0) >= max_samples:\n",
    "                break\n",
    "    \n",
    "    all_z = torch.cat(all_z, dim=0)[:max_samples].numpy()\n",
    "    all_labels = torch.cat(all_labels, dim=0)[:max_samples].numpy()\n",
    "    all_preds = torch.cat(all_preds, dim=0)[:max_samples].numpy()\n",
    "    \n",
    "    print(f\"Running t-SNE on {len(all_z)} samples...\")\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "    z_2d = tsne.fit_transform(all_z)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "    \n",
    "    for i in range(10):\n",
    "        mask = all_preds == i\n",
    "        ax.scatter(z_2d[mask, 0], z_2d[mask, 1], \n",
    "                  c=[colors[i]], \n",
    "                  label=f'{i}',\n",
    "                  alpha=0.7, \n",
    "                  s=25, \n",
    "                  edgecolors='white',\n",
    "                  linewidth=0.5)\n",
    "    \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    \n",
    "    ax.text(0.5, -0.05, f'(a) {title}', \n",
    "            transform=ax.transAxes,\n",
    "            ha='center', \n",
    "            fontsize=14, \n",
    "            fontweight='bold')\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f'cluster_distribution_{title.lower().replace(\" \", \"_\")}.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    print(f\"Cluster distribution saved to '{filename}'\")\n",
    "    plt.show()\n",
    "    \n",
    "    return z_2d, all_labels, all_preds\n",
    "\n",
    "\n",
    "def visualize_results(history, test_metrics):\n",
    "    \"\"\"Visualize training results\"\"\"\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1 = plt.subplot(1, 3, 1)\n",
    "    epochs = range(1, len(history['loss']) + 1)\n",
    "    ax1.plot(epochs, history['loss'], label='Total Loss', linewidth=2, marker='o', markersize=3)\n",
    "    ax1.plot(epochs, history['recon_loss'], label='Recon Loss', linewidth=2, marker='s', markersize=3)\n",
    "    ax1.plot(epochs, history['rc_loss'], label='RC Loss', linewidth=2, marker='^', markersize=3)\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', fontsize=12)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation metrics\n",
    "    ax2 = plt.subplot(1, 3, 2)\n",
    "    val_epochs = [1] + list(range(10, len(history['loss']) + 1, 10))\n",
    "    ax2.plot(val_epochs, history['val_acc'], label='ACC', linewidth=2, marker='o', markersize=6)\n",
    "    ax2.plot(val_epochs, history['val_nmi'], label='NMI', linewidth=2, marker='s', markersize=6)\n",
    "    ax2.plot(val_epochs, history['val_ari'], label='ARI', linewidth=2, marker='^', markersize=6)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Score', fontsize=12)\n",
    "    ax2.set_title('Validation Metrics', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim([0, 1])\n",
    "    \n",
    "    # Test metrics\n",
    "    ax3 = plt.subplot(1, 3, 3)\n",
    "    metric_names = list(test_metrics.keys())\n",
    "    metric_values = list(test_metrics.values())\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "    bars = ax3.bar(metric_names, metric_values, color=colors, alpha=0.7, \n",
    "                   edgecolor='black', linewidth=2)\n",
    "    ax3.set_ylabel('Score', fontsize=12)\n",
    "    ax3.set_title('Test Performance', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', \n",
    "                fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('drc_results.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nResults saved to 'drc_results.png'\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():   \n",
    "    INPUT_DIM = 784\n",
    "    HIDDEN_DIMS = [500, 500, 2000]\n",
    "    LATENT_DIM = 10\n",
    "    N_CLUSTERS = 10\n",
    "    BATCH_SIZE = 256\n",
    "    \n",
    "    PRETRAIN_EPOCHS = 50\n",
    "    TRAIN_EPOCHS = 200\n",
    "    LR = 0.0001\n",
    "    \n",
    "    GAMMA = 0.01 \n",
    "    V = 100.0    \n",
    "    \n",
    "    print(\"=== DRC ===\")\n",
    "    print(f\"Architecture: {INPUT_DIM} -> {HIDDEN_DIMS} -> {LATENT_DIM}\")\n",
    "    print(f\"Learning rate: {LR}\")\n",
    "    print(f\"Gamma: {GAMMA}\")\n",
    "    print(f\"Reward v: {V}\")\n",
    "    print(f\"Epochs: {TRAIN_EPOCHS}\")\n",
    "    \n",
    "    # Load MNIST\n",
    "    print(\"\\n=== Loading MNIST Dataset ===\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    \n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, \n",
    "                                   download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, \n",
    "                                  download=True, transform=transform)\n",
    "    \n",
    "    # Train/val split\n",
    "    train_size = int(0.9 * len(train_dataset))\n",
    "    val_size = len(train_dataset) - train_size\n",
    "    train_subset, val_subset = torch.utils.data.random_split(\n",
    "        train_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, \n",
    "                             shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, \n",
    "                           shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, \n",
    "                            shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Train: {len(train_subset)}, Val: {len(val_subset)}, Test: {len(test_dataset)}\")\n",
    "    \n",
    "    # model\n",
    "    model = DRC(INPUT_DIM, HIDDEN_DIMS, LATENT_DIM, N_CLUSTERS).to(device)\n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    full_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=2)\n",
    "    \n",
    "    pretrain_autoencoder(model, full_train_loader, PRETRAIN_EPOCHS, lr=0.001)\n",
    "    initialize_cluster_centers(model, full_train_loader, N_CLUSTERS)\n",
    "    \n",
    "    history = train_drc(model, train_loader, val_loader, \n",
    "                       epochs=TRAIN_EPOCHS, lr=LR, gamma=GAMMA, v=V)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"\\n=== Final Test Evaluation ===\")\n",
    "    test_metrics = evaluate_model(model, test_loader)\n",
    "    print(f\"Test ACC: {test_metrics['ACC']:.4f}\")\n",
    "    print(f\"Test NMI: {test_metrics['NMI']:.4f}\")\n",
    "    print(f\"Test ARI: {test_metrics['ARI']:.4f}\")\n",
    "    \n",
    "    visualize_results(history, test_metrics)\n",
    "    visualize_tsne_clusters(model, test_loader, max_samples=2000)\n",
    "    z_2d, labels, preds = visualize_cluster_distribution(\n",
    "        model, test_loader, \n",
    "        title=\"MNIST\"\n",
    "    )\n",
    "    return model, history, test_metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model, history, metrics = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
